{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline\n",
    "import random\n",
    "from pprint import pprint\n",
    "import time\n",
    "from joblib import Parallel, delayed, parallel_backend, Memory\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differents functions\n",
    "# spliting datas\n",
    "def train_test_split(data, test_size):\n",
    "    \n",
    "    #check if test_size is a float\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(data))\n",
    "    \n",
    "    #retrieve all indices\n",
    "    indices = data.index.tolist()\n",
    "    \n",
    "    #select random indices according to test_size\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "    \n",
    "    #spliting datas in training & testing\n",
    "    test_data = data.loc[test_indices]\n",
    "    train_data = data.drop(test_indices)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "# residuals calculation\n",
    "def compute_residuals(datas):\n",
    "    \n",
    "    cols = datas.columns.tolist()\n",
    "    l = len(cols) - 2 - INDEX_TARGET\n",
    "    datas[\"residual_\" + str(l)] = datas[cols[INDEX_TARGET]] - datas[cols[-1]]\n",
    "    \n",
    "    return datas\n",
    "\n",
    "# compute similarity score\n",
    "def similarity(data):\n",
    "    \n",
    "   \n",
    "    res = np.sum(data[:,-1])\n",
    "    prob = np.sum(data[:, -2] * (1-data[:,-2])) + 1\n",
    "    sim = (res**2)/(prob) \n",
    "    \n",
    "    #print(\"sim: {}\".format((res, prob, sim)))\n",
    "    \n",
    "    return sim\n",
    "\n",
    "# compute output for each leaf\n",
    "def output(data):\n",
    "    res = np.sum(data[:,-1])\n",
    "    prob = np.sum(data[:, -2] * (1-data[:,-2])) + 1\n",
    "    out = res/prob\n",
    "    \n",
    "    return out\n",
    "    \n",
    "#compute gain\n",
    "def get_gain(data, data_inf, data_sup):\n",
    "    \n",
    "    gain = similarity(data_inf) + similarity(data_sup) - similarity(data)\n",
    "    \n",
    "    return gain\n",
    "    \n",
    "#Normal classification using majority\n",
    "def classify(data):\n",
    "    #get all classes & count occurences\n",
    "    label = data[:,-1]\n",
    "    uniques_classes, counts = np.unique(data[:,-1], return_counts = True)\n",
    "    \n",
    "    #get index of most common class\n",
    "    index = counts.argmax()\n",
    "    \n",
    "    #return the most common class\n",
    "    return uniques_classes[index]\n",
    "\n",
    "#Calculate all potential splits\n",
    "def get_splits(data):\n",
    "    \n",
    "    #initialize our dict of potential splits\n",
    "    splits = {}\n",
    "    #initialize all attribute potential splits list\n",
    "    #_, n_cols = data.shape\n",
    "    \n",
    "    for col in range(INDEX_TARGET):\n",
    "        if col == 13:\n",
    "            continue\n",
    "        splits[col] = list()\n",
    "    \n",
    "    # Compute all attributes potential splits\n",
    "    for col in range(INDEX_TARGET):\n",
    "        #get unique datas\n",
    "        #print(\"Current col: {}\".format(col))\n",
    "        if col == 13:\n",
    "            continue\n",
    "        values = np.unique(data[:, col])\n",
    "        #populate our dict\n",
    "        feature_type = FEATURE_TYPES[col]\n",
    "        if feature_type == \"Continous\":\n",
    "            for index in range(1, len(values)):\n",
    "                current_value = values[index]\n",
    "                #print(current_value)\n",
    "                previous_value = values[index - 1]\n",
    "                potential_split = np.mean([current_value, previous_value])\n",
    "                splits[col].append(potential_split)\n",
    "        else:\n",
    "            splits[col] = values\n",
    "    return splits\n",
    "\n",
    "#Spliting data\n",
    "def split_data(data, feature_col, value):\n",
    "    \n",
    "    feature_type = FEATURE_TYPES[feature_col]\n",
    "    \n",
    "    #define all masks\n",
    "    \n",
    "    if feature_type == \"Continous\":\n",
    "        mask_inf = data[:, feature_col] <= value\n",
    "        mask_sup = data[:, feature_col] > value\n",
    "    else:\n",
    "        mask_inf = data[:, feature_col] == value\n",
    "        mask_sup = data[:, feature_col] != value\n",
    "    \n",
    "    #data spliting\n",
    "    data_inf = data[mask_inf]\n",
    "    data_sup = data[mask_sup]\n",
    "    \n",
    "    return data_inf, data_sup\n",
    "\n",
    "\n",
    "#determine best split attribute and value\n",
    "def determine_best_split(data, potential_splits):\n",
    "    \n",
    "    overall_metric_value = -300000000\n",
    "     \n",
    "    for colum_index in potential_splits:\n",
    "        #print(\"current column: {}\".format(colum_index))\n",
    "        for value in potential_splits[colum_index]:\n",
    "            #print(\"current value: {}\".format(value))\n",
    "            data_inf, data_sup = split_data(data, colum_index, value)\n",
    "            current_overall = get_gain(data, data_inf, data_sup)\n",
    "            #print(\"{}\".format((current_overall, overall_metric_value)))\n",
    "            \n",
    "            #print(current_overall)\n",
    "          #print(current_overall)\n",
    "          #check if lower\n",
    "            if current_overall >= overall_metric_value:\n",
    "                #print(\"cur,met, val, col, : ({}, {},{},{})\".format(current_overall, overall_metric_value, colum_index, value))\n",
    "                overall_metric_value = current_overall\n",
    "                best_split_column = colum_index\n",
    "                best_split_value = value\n",
    "    \n",
    "    #print(\"Final Done !! bests: ({}, {}, {})\".format(best_split_column, best_split_value, overall_metric_value))\n",
    "    return best_split_column, best_split_value, overall_metric_value\n",
    "\n",
    "#building decision Tree\n",
    "def decision_tree(df, counter=0, min_samples=5, max_depth=5, metric=\"entropy\"):\n",
    "    \n",
    "    if counter == 0:\n",
    "        data = df.values\n",
    "        global COLUMNS_NAMES, FEATURE_TYPES, METRIC, GAIN\n",
    "        GAIN = 0\n",
    "        COLUMNS_NAMES = df.columns[:INDEX_TARGET]\n",
    "        FEATURE_TYPES = determine_feature_types(df)\n",
    "        METRIC = metric\n",
    "    else:\n",
    "        data = df\n",
    "        \n",
    "    #base case\n",
    "    \n",
    "    if (len(data) < min_samples) or (counter == max_depth) or (GAIN < 0):\n",
    "        classification = output(data)\n",
    "        return classification\n",
    "    \n",
    "    else:\n",
    "        counter +=1\n",
    "        #computations for right and left part\n",
    "        potential_splits = get_splits(data)\n",
    "        best_split_column, best_split_value, overall_metric_value = determine_best_split(data, potential_splits)\n",
    "        GAIN = overall_metric_value\n",
    "        #We must change data_inf & data_sup order later\n",
    "        data_inf, data_sup = split_data(data, best_split_column, best_split_value)\n",
    "        \n",
    "        # Creating subTree\n",
    "        feature_type = FEATURE_TYPES[best_split_column]    \n",
    "        if feature_type == \"Continous\":\n",
    "            question = \"{} <= {}\".format(COLUMNS_NAMES[best_split_column], best_split_value)\n",
    "        else:\n",
    "            question = \"{} == {}\".format(COLUMNS_NAMES[best_split_column], best_split_value)\n",
    "        #Adding labels\n",
    "        labels = \" \" + str(len(data_inf) + len(data_sup)) + \" \" + str(overall_metric_value)\n",
    "        question += labels\n",
    "        sub_tree = {question: []}\n",
    "        #print(question)\n",
    "            \n",
    "        \n",
    "        #left and right\n",
    "        yes_answer = decision_tree(data_inf, counter, min_samples, max_depth, metric=METRIC)\n",
    "        no_answer = decision_tree(data_sup, counter, min_samples, max_depth, metric=METRIC)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            print(yes_answer)\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            #Append left and right part\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree\n",
    "        \n",
    "#classify a sample\n",
    "def classify_sample(sample, tree):\n",
    "    \n",
    "    #get node elements\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    key = list(tree.keys())[0]\n",
    "    #print(key.split())\n",
    "    feature, comp_op, value, _, _ = key.split()\n",
    "    \n",
    "    if comp_op == \"<=\":\n",
    "        if sample[feature] <= float(value):\n",
    "            answer = tree[key][0]\n",
    "        else:\n",
    "            answer = tree[key][1]\n",
    "    else:\n",
    "        if str(sample[feature]) == value:\n",
    "            answer = tree[key][0]\n",
    "        else:\n",
    "            answer = tree[key][1]\n",
    "        \n",
    "    #test base case\n",
    "    if not isinstance(answer, dict):\n",
    "        #print('yes')\n",
    "        return answer\n",
    "    else:\n",
    "        return classify_sample(sample, answer)\n",
    "    \n",
    "# Pruning tree\n",
    "def pruning(tree, datas):\n",
    "    \n",
    "    node = list(tree.keys())[0]\n",
    "    left_leaf, right_leaf = tree[node]\n",
    "    \n",
    "    if not isinstance(left_leaf, dict) and not isinstance(right_leaf, dict):\n",
    "        print('yes')\n",
    "    \n",
    "\n",
    "#compute accuracy\n",
    "def compute_predictions(df, tree):\n",
    "    \n",
    "    learning_rate = 0.3\n",
    "    \n",
    "    # datas columns list\n",
    "    cols =  df.columns.tolist()\n",
    "    \n",
    "    # last predictions column\n",
    "    if TEST == False:\n",
    "        last_pred = cols[-2]\n",
    "    else:\n",
    "        last_pred = cols[-1]\n",
    "    \n",
    "    # compute the next prediction index\n",
    "    l = len(cols) + 1\n",
    "    \n",
    "    # Get all ouputs\n",
    "    outputs = df.apply(classify_sample, axis=1, args=(tree,))\n",
    "    \n",
    "    # Compute log(odds)\n",
    "    log_odds = np.log((df[last_pred] / (1-df[last_pred]))) + outputs*learning_rate\n",
    "    \n",
    "    final_pred = np.exp(log_odds)/(1 + np.exp(log_odds))\n",
    "    \n",
    "    if TEST == False:\n",
    "        pred_name = \"pred_\" + str(l)\n",
    "        df[pred_name] = final_pred\n",
    "    else:\n",
    "        pred_name = \"classification\"\n",
    "        df[pred_name] = final_pred\n",
    "        df[pred_name] = df[pred_name].apply(convert_predictions)\n",
    "        df[\"classification_correct\"] = df.classification == df.A15\n",
    "    \n",
    "    \n",
    "    #accuracy = df.classification_correct.mean()\n",
    "    return df\n",
    "    \n",
    "#convert predictions\n",
    "def convert_predictions(val):\n",
    "    \n",
    "    return (1, 0)[val <= 0.5]\n",
    "\n",
    "# In order to handle non-continous values\n",
    "# We have to identify all features type in our dataset\n",
    "def determine_feature_types(data):\n",
    "    \n",
    "    features_type  = []\n",
    "    threshold = 931\n",
    "    for col in data.columns:\n",
    "        \n",
    "        uniques_val = data[col].unique()\n",
    "        sample = uniques_val[0]\n",
    "        \n",
    "        if (isinstance(sample, str)):\n",
    "            features_type.append(\"Categorical\")\n",
    "        else:\n",
    "            features_type.append(\"Continous\")\n",
    "    \n",
    "    return features_type\n",
    "\n",
    "#boosting algorithm\n",
    "def boosting (train, test, n_estimators,sample_size, validation_size, early_stopping):\n",
    "    \n",
    "    # Set sampling size\n",
    "    size = round(len(train)*sample_size)\n",
    "    \n",
    "    # Divide \n",
    "    train, val = train_test_split(train, validation_size)\n",
    "    eraly_in = 0\n",
    "    best_acc = 0\n",
    "    best_tree = 0.5\n",
    "    \n",
    "    \n",
    "    global TEST\n",
    "    \n",
    "    # Training phase , set test operation to false\n",
    "    TEST = False \n",
    "    \n",
    "    for k in range(n_estimators):\n",
    "        \n",
    "        print(\"step: {} left: {}\".format(k+1, n_estimators-k-1))\n",
    "        \n",
    "        # residuals computation\n",
    "        train = compute_residuals(train)\n",
    "        train_sample = train.sample(size, replace=True)\n",
    "        \n",
    "        #get cover value\n",
    "        #cover = round(np.sum(train.values[:, -2] * (1-train.values[:,-2])) - 1)\n",
    "        \n",
    "        #build tree\n",
    "        my_tree = decision_tree(train_sample, min_samples=5, max_depth=5)\n",
    "        \n",
    "        #build new predictions\n",
    "        train = compute_predictions(train, my_tree)\n",
    "        \n",
    "        # Validation set evaluation\n",
    "        \n",
    "        TEST = True\n",
    "        val_test = compute_predictions(val.copy(), my_tree)\n",
    "        \n",
    "        res = compute_metrics(val_test)\n",
    "        \n",
    "        acc = res[0][0]\n",
    "        \n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_tree = my_tree\n",
    "            early_in = 0\n",
    "        else:\n",
    "            early_in += 1\n",
    "            \n",
    "        if early_in == early_stopping:\n",
    "            break\n",
    "        print(\" acc: {} early: {}\".format(best_acc, early_in))\n",
    "            \n",
    "        TEST = False\n",
    "            \n",
    "        #print(train)\n",
    "        \n",
    "    \n",
    "    # Testing phase , set test operation to false\n",
    "    TEST = True\n",
    "    test = compute_predictions(test, best_tree)\n",
    "    \n",
    "    \n",
    "    return best_tree, train, test\n",
    "\n",
    "def compute_metrics(df):\n",
    "    \n",
    "    # 1- Accuracy\n",
    "    accuracy = df.classification_correct.mean()\n",
    "    \n",
    "    #2- Recall, precision\n",
    "    TP = len(df[(df.A15 == 1) & (df.classification == 1)])\n",
    "    FP = len(df[(df.A15 == 0) & (df.classification == 1)])\n",
    "    FN = len(df[(df.A15 == 1) & (df.classification == 0)])\n",
    "    TN = len(df[(df.A15 == 0) & (df.classification == 0)])\n",
    "\n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    Precision, Recall\n",
    "    \n",
    "    #3- Lost, no_win\n",
    "    P = df.A13[(df.A15 == 0) & (df.classification == 1)].sum()\n",
    "\n",
    "    df_no_win = df[[\"A13\", \"A3\", \"A5\"]][(df.A15 == 1) & (df.classification == 0)]\n",
    "    df_no_win['A3'] /=100\n",
    "    df_no_win[\"NoWin\"] = df_no_win.A13 * df_no_win.A3 * df_no_win.A5/12\n",
    "\n",
    "    M = df_no_win.NoWin.sum()\n",
    "    P, M\n",
    "    \n",
    "    4# Error\n",
    "    error = (FP / (TP + FP)) + (FN / FN + TN)\n",
    "    \n",
    "    #Friedman Test\n",
    "    #f, _ = friedmanchisquare(df.kredit.values, df.classification.values)\n",
    "    \n",
    "    #resulting metric list\n",
    "    result = [(accuracy, Precision, Recall, error), (P, M), (TP, FP, FN, TN)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Execution\n",
    "random.seed(15)\n",
    "datas = pd.read_csv('australian.dat',sep=' ')\n",
    "global INDEX_TARGET\n",
    "INDEX_TARGET = 14\n",
    "\n",
    "datas['pred_0'] = 0.5\n",
    "train, test = train_test_split(datas, 0.3)\n",
    "my_tree, new_train, new_test = boosting(train, test, 100, 0.5, 0.3, 10)\n",
    "compute_metrics(new_test)\n",
    "new_test.to_excel('classic_boosting_2.xlsx', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
