{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline\n",
    "import random\n",
    "from pprint import pprint\n",
    "import time\n",
    "from joblib import Parallel, delayed, parallel_backend, Memory\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differents functions\n",
    "# spliting datas\n",
    "def train_test_split(data, test_size):\n",
    "    \n",
    "    #check if test_size is a float\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(data))\n",
    "    \n",
    "    #retrieve all indices\n",
    "    indices = data.index.tolist()\n",
    "    \n",
    "    #select random indices according to test_size\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "    \n",
    "    #spliting datas in training & testing\n",
    "    test_data = data.loc[test_indices]\n",
    "    train_data = data.drop(test_indices)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "\n",
    "#Check data purity\n",
    "def check_purity(data):\n",
    "    #Return purity value regarding to the number of unique classes\n",
    "    return  (False,True)[len(np.unique(data[:,-1])) == 1]\n",
    "\n",
    "\n",
    "#Normal classification using majority\n",
    "def classify(data):\n",
    "    #get all classes & count occurences\n",
    "    label = data[:,-1]\n",
    "    uniques_classes, counts = np.unique(data[:,-1], return_counts = True)\n",
    "    \n",
    "    #get index of most common class\n",
    "    index = counts.argmax()\n",
    "    \n",
    "    #return the most common class\n",
    "    return uniques_classes[index]\n",
    "\n",
    "\n",
    "# Modified Classification using proposed approach\n",
    "def modified_classify(data):\n",
    "    #initializing\n",
    "    P  = 0\n",
    "    M = 0\n",
    "    n_row , _ = data.shape\n",
    "    index_montant = 0\n",
    "    index_taux = 4\n",
    "    index_nbe = 3\n",
    "    for line in range(n_row):\n",
    "        if data[line,-1] == 0:\n",
    "            P += data[line,index_montant]\n",
    "        else:\n",
    "            M += (data[line, index_taux]/100) * data[line,index_montant]\n",
    "        #print(\"({} , {})\".format(data[line,index_montant], data[line, index_taux]))\n",
    "    \n",
    "    #print(\"({}, {})\".format(P, M))\n",
    "    \n",
    "    return (0, 1)[M > P]\n",
    "        \n",
    "\n",
    "#Calculate all potential splits\n",
    "def get_splits(data):\n",
    "    \n",
    "    #initialize our dict of potential splits\n",
    "    splits = {}\n",
    "    \n",
    "    #initialize all attribute potential splits list\n",
    "    _, n_cols = data.shape\n",
    "    for col in range(n_cols - 1):\n",
    "        if col == 0:\n",
    "            continue\n",
    "        splits[col] = list()\n",
    "    \n",
    "    # Compute all attributes potential splits\n",
    "    for col in range(n_cols - 1):\n",
    "        if col == 0:\n",
    "            continue\n",
    "        #get unique datas\n",
    "        #print(\"Current col: {}\".format(col))\n",
    "        values = np.unique(data[:, col])\n",
    "        #populate our dict\n",
    "        feature_type = FEATURE_TYPES[col]\n",
    "        if feature_type == \"Continous\":\n",
    "            for index in range(1, len(values)):\n",
    "                current_value = values[index]\n",
    "                #print(current_value)\n",
    "                previous_value = values[index - 1]\n",
    "                potential_split = np.mean([current_value, previous_value])\n",
    "                splits[col].append(potential_split)\n",
    "        else:\n",
    "            splits[col] = values\n",
    "    return splits\n",
    "\n",
    "\n",
    "#Spliting data\n",
    "def split_data(data, feature_col, value):\n",
    "    \n",
    "    feature_type = FEATURE_TYPES[feature_col]\n",
    "    \n",
    "    #define all masks\n",
    "    \n",
    "    if feature_type == \"Continous\":\n",
    "        mask_inf = data[:, feature_col] <= value\n",
    "        mask_sup = data[:, feature_col] > value\n",
    "    else:\n",
    "        mask_inf = data[:, feature_col] == value\n",
    "        mask_sup = data[:, feature_col] != value\n",
    "    \n",
    "    #data spliting\n",
    "    data_inf = data[mask_inf]\n",
    "    data_sup = data[mask_sup]\n",
    "    \n",
    "    return data_inf, data_sup\n",
    "\n",
    "\n",
    "#Calculate chosen metric\n",
    "def calculate_metric(data):\n",
    "     #get classes\n",
    "    label_class = data[:, -1]\n",
    "    #get counts for each class\n",
    "    _, counts = np.unique(label_class, return_counts=True)\n",
    "    \n",
    "    probabilities = counts / counts.sum()\n",
    "    \n",
    "    #computing metric value depending on the user choice\n",
    "    if METRIC == \"entropy\":\n",
    "        probabilities = counts / counts.sum()\n",
    "        computed_metric = sum(probabilities * -np.log2(probabilities))\n",
    "    elif METRIC == \"gini\":\n",
    "        probabilities **=2\n",
    "        computed_metric = 1- sum(probabilities)\n",
    "    \n",
    "    return computed_metric\n",
    "\n",
    "\n",
    "# Overall metric value\n",
    "def overall_metric(data_inf, data_sup):\n",
    "    #get number of datas\n",
    "    data_all_lenght = len(data_inf) + len(data_sup)\n",
    "    \n",
    "    #compute overall metric value\n",
    "    metric_data_inf = (len(data_inf) / data_all_lenght)*calculate_metric(data_inf)\n",
    "    metric_data_sup = (len(data_sup) / data_all_lenght)*calculate_metric(data_sup)\n",
    "    overall_metric= metric_data_inf + metric_data_sup\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "\n",
    "#Modified metric: Here the aim is to compute the total lost that we'll have\n",
    "def modified_metric(data):\n",
    "    #initializing\n",
    "    P  = 0\n",
    "    M = 0\n",
    "    n_row , _ = data.shape\n",
    "    index_montant = 0\n",
    "    index_taux = 4\n",
    "    index_nbe = 3\n",
    "    for line in range(n_row):\n",
    "        if data[line,-1] == 0:\n",
    "            P += data[line,index_montant]\n",
    "        else:\n",
    "            M += (data[line, index_taux]/100) * data[line,index_montant]\n",
    "    #print(P)\n",
    "    return P\n",
    "\n",
    "\n",
    "#Compute our overall modified metric\n",
    "def overall_modified_metric(data_inf, data_sup):\n",
    "    #get number of datas\n",
    "    data_all_lenght = len(data_inf) + len(data_sup)\n",
    "    \n",
    "    #compute overall metric value\n",
    "    metric_data_inf = (len(data_inf) / data_all_lenght)*modified_metric(data_inf)\n",
    "    metric_data_sup = (len(data_sup) / data_all_lenght)*modified_metric(data_sup)\n",
    "    overall_metric= metric_data_inf + metric_data_sup\n",
    "    #print(\"({}, {})\".format(metric_data_inf, metric_data_sup))\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "\n",
    "#parralelized function\n",
    "def computing_best_column(data, colum_index, value):\n",
    "    \n",
    "    global best_split_column, best_split_value, overall_metric_value\n",
    "    \n",
    "    #print(\"current column: {}\".format(colum_index))\n",
    "    data_inf, data_sup = split_data(data, colum_index, value)\n",
    "    current_overall = overall_metric(data_inf, data_sup)\n",
    "            #print(current_overall)\n",
    "            #check if lower\n",
    "    if current_overall <= overall_metric_value:\n",
    "        print(\"cur,met, val, col, : ({}, {},{},{})\".format(current_overall, overall_metric_value, colum_index, value))\n",
    "        overall_metric_value = current_overall\n",
    "        best_split_column = colum_index\n",
    "        best_split_value = value\n",
    "        #print(\"Done !! bests: ({}, {}, {})\".format(best_split_column, best_split_value, overall_metric_value))\n",
    "\n",
    "        \n",
    "#determine best split attribute and value\n",
    "def determine_best_split(data, potential_splits):\n",
    "     \n",
    "    #print(potential_splits)\n",
    "    \n",
    "    #So let's implement parralel version of our super code\n",
    "    #Parallel(n_jobs=-1, require='sharedmem')(\n",
    "     #   delayed(func)(args)\n",
    "    #)\n",
    "    global best_split_column, best_split_value, overall_metric_value\n",
    "    \n",
    "    overall_metric_value = 300000000000000\n",
    "    \n",
    "    with Parallel(n_jobs=3, backend=\"threading\", require=\"sharedmem\", verbose=5) as parallel:\n",
    "        parallel(delayed(computing_best_column)(data, colum_index, value) for colum_index in potential_splits for value in potential_splits[colum_index]) \n",
    "    \n",
    "    \"\"\"\n",
    "        for colum_index in potential_splits:\n",
    "        print(\"current column: {}\".format(colum_index))\n",
    "        for value in potential_splits[colum_index]:\n",
    "            print(\"current value: {}\".format(value))\n",
    "            data_inf, data_sup = split_data(data, colum_index, value)\n",
    "            current_overall = overall_metric(data_inf, data_sup)\n",
    "            #print(current_overall)\n",
    "            #check if lower\n",
    "            if current_overall <= overall_metric_value:\n",
    "                overall_metric_value = current_overall\n",
    "                best_split_column = colum_index\n",
    "                best_split_value = value\n",
    "    #print(best_split_value)\n",
    "    \"\"\"\n",
    "    #Loop over all datas, calculate overall_entropy, and update if it's lower\n",
    "    \n",
    "    print(\"Final Done !! bests: ({}, {}, {})\".format(best_split_column, best_split_value, overall_metric_value))\n",
    "    return best_split_column, best_split_value, overall_metric_value\n",
    "\n",
    "\n",
    "#building decision Tree\n",
    "def decision_tree(df, counter=0, min_samples=5, max_depth=5, metric=\"entropy\"):\n",
    "    \n",
    "    if counter == 0:\n",
    "        data = df.values\n",
    "        global COLUMNS_NAMES, FEATURE_TYPES, METRIC\n",
    "        COLUMNS_NAMES = df.columns[:-1]\n",
    "        FEATURE_TYPES = determine_feature_types(df)\n",
    "        METRIC = metric\n",
    "    else:\n",
    "        data = df\n",
    "        \n",
    "    #base case\n",
    "    \n",
    "    if check_purity(data) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify(data)\n",
    "        return classification\n",
    "    \n",
    "    else:\n",
    "        counter +=1\n",
    "        #computations for right and left part\n",
    "        potential_splits = get_splits(data)\n",
    "        best_split_column, best_split_value, overall_metric_value = determine_best_split(data, potential_splits)\n",
    "        #We must change data_inf & data_sup order later\n",
    "        data_inf, data_sup = split_data(data, best_split_column, best_split_value)\n",
    "        \n",
    "        # Creating subTree\n",
    "        feature_type = FEATURE_TYPES[best_split_column]    \n",
    "        if feature_type == \"Continous\":\n",
    "            question = \"{} <= {}\".format(COLUMNS_NAMES[best_split_column], best_split_value)\n",
    "        else:\n",
    "            question = \"{} == {}\".format(COLUMNS_NAMES[best_split_column], best_split_value)\n",
    "        #Adding labels\n",
    "        labels = \" \" + str(len(data_inf) + len(data_sup)) + \" \" + str(overall_metric_value)\n",
    "        question += labels\n",
    "        sub_tree = {question: []}\n",
    "        print(question)\n",
    "            \n",
    "        \n",
    "        #left and right\n",
    "        yes_answer = decision_tree(data_inf, counter, min_samples, max_depth, metric=METRIC)\n",
    "        no_answer = decision_tree(data_sup, counter, min_samples, max_depth, metric=METRIC)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            print(yes_answer)\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            #Append left and right part\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree\n",
    "\n",
    "    \n",
    "#classify a sample\n",
    "def classify_sample(sample, tree):\n",
    "    \n",
    "    #get node elements\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    key = list(tree.keys())[0]\n",
    "    #print(key.split())\n",
    "    feature, comp_op, value, _, _ = key.split()\n",
    "    \n",
    "    if comp_op == \"<=\":\n",
    "        if sample[feature] <= float(value):\n",
    "            answer = tree[key][0]\n",
    "        else:\n",
    "            answer = tree[key][1]\n",
    "    else:\n",
    "        if str(sample[feature]) == value:\n",
    "            answer = tree[key][0]\n",
    "        else:\n",
    "            answer = tree[key][1]\n",
    "        \n",
    "    #test base case\n",
    "    if not isinstance(answer, dict):\n",
    "        #print('yes')\n",
    "        return answer\n",
    "    else:\n",
    "        return classify_sample(sample, answer)\n",
    "\n",
    "    \n",
    "#compute accuracy\n",
    "def my_accuracy(df, tree):\n",
    "    \n",
    "    df[\"classification\"] = df.apply(classify_sample, axis=1, args=(tree,))\n",
    "    df[\"classification_correct\"] = df.classification == df.loan_status\n",
    "    \n",
    "    accuracy = df.classification_correct.mean()\n",
    "    \n",
    "    return accuracy, df\n",
    "    \n",
    "\n",
    "# In order to handle non-continous values\n",
    "# We have to identify all features type in our dataset\n",
    "def determine_feature_types(data):\n",
    "    \n",
    "    features_type  = []\n",
    "    threshold = 931\n",
    "    for col in data.columns:\n",
    "        \n",
    "        uniques_val = data[col].unique()\n",
    "        sample = uniques_val[0]\n",
    "        \n",
    "        if (isinstance(sample, str)):\n",
    "            features_type.append(\"Categorical\")\n",
    "        else:\n",
    "            features_type.append(\"Continous\")\n",
    "    \n",
    "    return features_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Execution\n",
    "\n",
    "#random.seed(30)\n",
    "#datas = pd.read_csv('datas/SouthGermanCredit.asc', sep=' ')\n",
    "\n",
    "datas = pd.read_excel('datas_loan.xlsx', index=False)\n",
    "datas.emp_length = datas.emp_length.astype(str)\n",
    "datas = datas.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "train, test = train_test_split(datas, 0.3)\n",
    "start = time.time()\n",
    "my_tree = decision_tree(train, min_samples=60, max_depth=5, metric=\"gini\")\n",
    "acc, df = my_accuracy(test, my_tree)\n",
    "end = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11949000, 8104.38, 20200)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing new matrix confusion values\n",
    "P = df.loan_amount[(df.loan_status == 0) & (df.classification == 1)].sum()\n",
    "R = df.loan_amount[(df.loan_status == 1) & (df.classification == 0)].sum()\n",
    "\n",
    "df_no_win = df[[\"loan_amount\", \"interest_rate\", \"term\"]][(df.loan_status == 1) & (df.classification == 0)]\n",
    "\n",
    "df_no_win['interest_rate'] /=100.0\n",
    "df_no_win[\"NoWin\"] = df_no_win.loan_amount * df_no_win.interest_rate * df_no_win.term/12\n",
    "\n",
    "M = df_no_win.NoWin.sum()\n",
    "P, M, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TP = len(df[(df.loan_default == 1) & (df.classification == 1)])\n",
    "FP = len(df[(df.loan_default == 0) & (df.classification == 1)])\n",
    "FN = len(df[(df.loan_default == 1) & (df.classification == 0)])\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "Precision, Recall"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
