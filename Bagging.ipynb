{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "#%matplotlib inline\n",
    "import random\n",
    "from pprint import pprint\n",
    "import time\n",
    "from joblib import Parallel, delayed, parallel_backend, Memory\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Differents functions\n",
    "# spliting datas\n",
    "def train_test_split(data, test_size):\n",
    "    \n",
    "    #check if test_size is a float\n",
    "    if isinstance(test_size, float):\n",
    "        test_size = round(test_size * len(data))\n",
    "    \n",
    "    #retrieve all indices\n",
    "    indices = data.index.tolist()\n",
    "    \n",
    "    #select random indices according to test_size\n",
    "    test_indices = random.sample(population=indices, k=test_size)\n",
    "    \n",
    "    #spliting datas in training & testing\n",
    "    test_data = data.loc[test_indices]\n",
    "    train_data = data.drop(test_indices)\n",
    "    \n",
    "    return train_data, test_data\n",
    "\n",
    "#Check data purity\n",
    "def check_purity(data):\n",
    "    #Return purity value regarding to the number of unique classes\n",
    "    return  (False,True)[len(np.unique(data[:,-1])) == 1]\n",
    "    \n",
    "#Normal classification using majority\n",
    "def classify(data):\n",
    "    #get all classes & count occurences\n",
    "    label = data[:,-1]\n",
    "    uniques_classes, counts = np.unique(data[:,-1], return_counts = True)\n",
    "    \n",
    "    #get index of most common class\n",
    "    index = counts.argmax()\n",
    "    \n",
    "    #return the most common class\n",
    "    return uniques_classes[index]\n",
    "\n",
    "# Modified Classification using proposed approach\n",
    "def modified_classify(data):\n",
    "    #initializing\n",
    "    P  = 0\n",
    "    M = 0\n",
    "    n_row , _ = data.shape\n",
    "    index_montant = 8\n",
    "    index_taux = 10\n",
    "    index_nbe = 9\n",
    "    for line in range(n_row):\n",
    "        if data[line,-1] == 'IMPAYE':\n",
    "            P += data[line,index_montant]\n",
    "        else:\n",
    "            M += (data[line, index_taux]/100) * data[line,index_montant] * data[line,index_nbe]/12\n",
    "        #print(\"({} , {})\".format(data[line,index_montant], data[line, index_taux]))\n",
    "    \n",
    "    #print(\"({}, {})\".format(P, M))\n",
    "    \n",
    "    return ('IMPAYE', 'PAYE')[M > P]\n",
    "        \n",
    "\n",
    "#Calculate all potential splits\n",
    "def get_splits(data):\n",
    "    \n",
    "    #initialize our dict of potential splits\n",
    "    splits = {}\n",
    "    \n",
    "    #initialize all attribute potential splits list\n",
    "    _, n_cols = data.shape\n",
    "    for col in range(n_cols - 1):\n",
    "        if col == 8:\n",
    "            continue\n",
    "        splits[col] = list()\n",
    "    \n",
    "    # Compute all attributes potential splits\n",
    "    for col in range(n_cols - 1):\n",
    "        #get unique datas\n",
    "        #print(\"Current col: {}\".format(col))\n",
    "        if col == 8:\n",
    "            continue\n",
    "        values = np.unique(data[:, col])\n",
    "        #populate our dict\n",
    "        feature_type = FEATURE_TYPES[col]\n",
    "        if feature_type == \"Continous\":\n",
    "            for index in range(1, len(values)):\n",
    "                current_value = values[index]\n",
    "                #print(current_value)\n",
    "                previous_value = values[index - 1]\n",
    "                potential_split = np.mean([current_value, previous_value])\n",
    "                splits[col].append(potential_split)\n",
    "        else:\n",
    "            splits[col] = values\n",
    "    return splits\n",
    "\n",
    "#Spliting data\n",
    "def split_data(data, feature_col, value):\n",
    "    \n",
    "    feature_type = FEATURE_TYPES[feature_col]\n",
    "    \n",
    "    #define all masks\n",
    "    \n",
    "    if feature_type == \"Continous\":\n",
    "        mask_inf = data[:, feature_col] <= value\n",
    "        mask_sup = data[:, feature_col] > value\n",
    "    else:\n",
    "        mask_inf = data[:, feature_col] == value\n",
    "        mask_sup = data[:, feature_col] != value\n",
    "    \n",
    "    #data spliting\n",
    "    data_inf = data[mask_inf]\n",
    "    data_sup = data[mask_sup]\n",
    "    \n",
    "    return data_inf, data_sup\n",
    "\n",
    "\n",
    "#Calculate chosen metric\n",
    "def calculate_metric(data):\n",
    "     #get classes\n",
    "    label_class = data[:, -1]\n",
    "    #get counts for each class\n",
    "    _, counts = np.unique(label_class, return_counts=True)\n",
    "    \n",
    "    probabilities = counts / counts.sum()\n",
    "    \n",
    "    #computing metric value depending on the user choice\n",
    "    if METRIC == \"entropy\":\n",
    "        probabilities = counts / counts.sum()\n",
    "        computed_metric = sum(probabilities * -np.log2(probabilities))\n",
    "    elif METRIC == \"gini\":\n",
    "        probabilities **=2\n",
    "        computed_metric = 1- sum(probabilities)\n",
    "    \n",
    "    return computed_metric\n",
    "\n",
    "# Overall metric value\n",
    "def overall_metric(data_inf, data_sup):\n",
    "    #get number of datas\n",
    "    data_all_lenght = len(data_inf) + len(data_sup)\n",
    "    \n",
    "    #compute overall metric value\n",
    "    metric_data_inf = (len(data_inf) / data_all_lenght)*calculate_metric(data_inf)\n",
    "    metric_data_sup = (len(data_sup) / data_all_lenght)*calculate_metric(data_sup)\n",
    "    overall_metric= metric_data_inf + metric_data_sup\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "#Modified metric: Here the aim is to compute the total lost that we'll have\n",
    "def modified_metric(data):\n",
    "    #initializing\n",
    "    P  = 0\n",
    "    M = 0\n",
    "    n_row , _ = data.shape\n",
    "    index_montant = 8\n",
    "    index_taux = 10\n",
    "    index_nbe = 9\n",
    "    for line in range(n_row):\n",
    "        if data[line,-1] == 'IMPAYE':\n",
    "            P += data[line,index_montant]\n",
    "        #else:\n",
    "            #M += (data[line, index_taux]/100) * data[line,index_montant] \n",
    "    #print(P)\n",
    "    return P\n",
    "\n",
    "#Compute our overall modified metric\n",
    "def overall_modified_metric(data_inf, data_sup):\n",
    "    #get number of datas\n",
    "    data_all_lenght = len(data_inf) + len(data_sup)\n",
    "    \n",
    "    #compute overall metric value\n",
    "    metric_data_inf = (len(data_inf) / data_all_lenght)*modified_metric(data_inf)\n",
    "    metric_data_sup = (len(data_sup) / data_all_lenght)*modified_metric(data_sup)\n",
    "    overall_metric= metric_data_inf + metric_data_sup\n",
    "    #print(\"({}, {})\".format(metric_data_inf, metric_data_sup))\n",
    "    \n",
    "    return overall_metric\n",
    "\n",
    "#parralelized function\n",
    "def computing_best_column(data, colum_index, value):\n",
    "    \n",
    "    global best_split_column, best_split_value, overall_metric_value\n",
    "    \n",
    "    #print(\"current column: {}\".format(colum_index))\n",
    "    data_inf, data_sup = split_data(data, colum_index, value)\n",
    "    current_overall = overall_modified_metric(data_inf, data_sup)\n",
    "            #print(current_overall)\n",
    "            #check if lower\n",
    "    if current_overall <= overall_metric_value:\n",
    "        print(\"cur,met, val, col, : ({}, {},{},{})\".format(current_overall, overall_metric_value, colum_index, value))\n",
    "        overall_metric_value = current_overall\n",
    "        best_split_column = colum_index\n",
    "        best_split_value = value\n",
    "        #print(\"Done !! bests: ({}, {}, {})\".format(best_split_column, best_split_value, overall_metric_value))\n",
    "\n",
    "#determine best split attribute and value\n",
    "def determine_best_split(data, potential_splits):\n",
    "     \n",
    "    #print(potential_splits)\n",
    "    \n",
    "    #So let's implement parralel version of our super code\n",
    "    #Parallel(n_jobs=-1, require='sharedmem')(\n",
    "     #   delayed(func)(args)\n",
    "    #)\n",
    "    #global best_split_column, best_split_value, overall_metric_value\n",
    "    \n",
    "    overall_metric_value = 300000000000000\n",
    "    \n",
    "    #with Parallel(n_jobs=-1, backend=\"threading\", require=\"sharedmem\", verbose=5) as parallel:\n",
    "      #  parallel(delayed(computing_best_column)(data, colum_index, value) for colum_index in potential_splits for value in potential_splits[colum_index]) \n",
    "    \n",
    "    for colum_index in potential_splits:\n",
    "        #print(\"current column: {}\".format(colum_index))\n",
    "        for value in potential_splits[colum_index]:\n",
    "            #print(\"current value: {}\".format(value))\n",
    "            data_inf, data_sup = split_data(data, colum_index, value)\n",
    "            current_overall = overall_metric(data_inf, data_sup)\n",
    "          #print(current_overall)\n",
    "          #check if lower\n",
    "            if current_overall <= overall_metric_value:\n",
    "                print(\"cur,met, val, col, : ({}, {},{},{})\".format(current_overall, overall_metric_value, colum_index, value))\n",
    "                overall_metric_value = current_overall\n",
    "                best_split_column = colum_index\n",
    "                best_split_value = value\n",
    "    #print(best_split_value)\n",
    "    #Loop over all datas, calculate overall_entropy, and update if it's lower\n",
    "    \n",
    "    print(\"Final Done !! bests: ({}, {}, {})\".format(best_split_column, best_split_value, overall_metric_value))\n",
    "    return best_split_column, best_split_value, overall_metric_value\n",
    "\n",
    "#building decision Tree\n",
    "def decision_tree(df, counter=0, min_samples=5, max_depth=5, metric=\"entropy\"):\n",
    "    \n",
    "    if counter == 0:\n",
    "        data = df.values\n",
    "        global COLUMNS_NAMES, FEATURE_TYPES, METRIC\n",
    "        COLUMNS_NAMES = df.columns[:-1]\n",
    "        FEATURE_TYPES = determine_feature_types(df)\n",
    "        METRIC = metric\n",
    "    else:\n",
    "        data = df\n",
    "        \n",
    "    #base case\n",
    "    \n",
    "    if check_purity(data) or (len(data) < min_samples) or (counter == max_depth):\n",
    "        classification = classify(data)\n",
    "        return classification\n",
    "    \n",
    "    else:\n",
    "        counter +=1\n",
    "        #computations for right and left part\n",
    "        potential_splits = get_splits(data)\n",
    "        best_split_column, best_split_value, overall_metric_value = determine_best_split(data, potential_splits)\n",
    "        #We must change data_inf & data_sup order later\n",
    "        data_inf, data_sup = split_data(data, best_split_column, best_split_value)\n",
    "        \n",
    "        # Creating subTree\n",
    "        feature_type = FEATURE_TYPES[best_split_column]    \n",
    "        if feature_type == \"Continous\":\n",
    "            question = \"{} <= {}\".format(COLUMNS_NAMES[best_split_column], best_split_value)\n",
    "        else:\n",
    "            question = \"{} == {}\".format(COLUMNS_NAMES[best_split_column], best_split_value)\n",
    "        #Adding labels\n",
    "        labels = \" \" + str(len(data_inf) + len(data_sup)) + \" \" + str(overall_metric_value)\n",
    "        question += labels\n",
    "        sub_tree = {question: []}\n",
    "        print(question)\n",
    "            \n",
    "        \n",
    "        #left and right\n",
    "        yes_answer = decision_tree(data_inf, counter, min_samples, max_depth, metric=METRIC)\n",
    "        no_answer = decision_tree(data_sup, counter, min_samples, max_depth, metric=METRIC)\n",
    "        \n",
    "        if yes_answer == no_answer:\n",
    "            print(yes_answer)\n",
    "            sub_tree = yes_answer\n",
    "        else:\n",
    "            #Append left and right part\n",
    "            sub_tree[question].append(yes_answer)\n",
    "            sub_tree[question].append(no_answer)\n",
    "        \n",
    "        return sub_tree\n",
    "        \n",
    "#classify a sample\n",
    "def classify_sample(sample, tree):\n",
    "    \n",
    "    #get node elements\n",
    "    if not isinstance(tree, dict):\n",
    "        return tree\n",
    "    key = list(tree.keys())[0]\n",
    "    #print(key.split())\n",
    "    feature, comp_op, value, _, _ = key.split()\n",
    "    \n",
    "    if comp_op == \"<=\":\n",
    "        if sample[feature] <= float(value):\n",
    "            answer = tree[key][0]\n",
    "        else:\n",
    "            answer = tree[key][1]\n",
    "    else:\n",
    "        if str(sample[feature]) == value:\n",
    "            answer = tree[key][0]\n",
    "        else:\n",
    "            answer = tree[key][1]\n",
    "        \n",
    "    #test base case\n",
    "    if not isinstance(answer, dict):\n",
    "        #print('yes')\n",
    "        return answer\n",
    "    else:\n",
    "        return classify_sample(sample, answer)\n",
    "\n",
    "#compute accuracy\n",
    "def my_accuracy(df, tree):\n",
    "    \n",
    "    df[\"classification\"] = df.apply(classify_sample, axis=1, args=(tree,))\n",
    "    df[\"classification_correct\"] = df.classification == df.ENIMPAYEOUPAS\n",
    "    \n",
    "    accuracy = df.classification_correct.mean()\n",
    "    \n",
    "    return accuracy, df\n",
    "    \n",
    "\n",
    "# In order to handle non-continous values\n",
    "# We have to identify all features type in our dataset\n",
    "def determine_feature_types(data):\n",
    "    \n",
    "    features_type  = []\n",
    "    threshold = 931\n",
    "    for col in data.columns:\n",
    "        \n",
    "        uniques_val = data[col].unique()\n",
    "        sample = uniques_val[0]\n",
    "        \n",
    "        if (isinstance(sample, str)):\n",
    "            features_type.append(\"Categorical\")\n",
    "        else:\n",
    "            features_type.append(\"Continous\")\n",
    "    \n",
    "    return features_type\n",
    "\n",
    "#Bagging process\n",
    "def bagging(train, test, metric, n_estimators, sample_size):\n",
    "    \n",
    "    decision_trees = dict()\n",
    "    size = round(len(train)*sample_size)\n",
    "    df_classes = pd.DataFrame()\n",
    "    test_final = test.copy()\n",
    "    #print(test_final.columns)\n",
    "    #Initialiazing estimators\n",
    "    for i in range(n_estimators):\n",
    "        decision_trees[i] = list()\n",
    "        \n",
    "        #Random samples\n",
    "        train_sample = train.sample(size, replace=True)\n",
    "        my_tree = decision_tree(train_sample, min_samples=60, max_depth=5, metric=metric)\n",
    "        acc, df = my_accuracy(test, my_tree)\n",
    "        decision_trees[i] = [my_tree, acc, df]\n",
    "        df_classes[\"estimator_class_\" + str(i)] = df.classification\n",
    "    #print(df_classes.columns)\n",
    "    print(test_final.columns)\n",
    "    test_final[\"classification\"] = df_classes.mode(axis=1)[0]\n",
    "    test_final[\"classification_correct\"] = test_final.classification == test_final.ENIMPAYEOUPAS\n",
    "    \n",
    "    return decision_trees, test_final\n",
    "\n",
    "def compute_metrics(df):\n",
    "    \n",
    "    # 1- Accuracy\n",
    "    accuracy = df.classification_correct.mean()\n",
    "    \n",
    "    #2- Recall, precision\n",
    "    TP = len(df[(df.ENIMPAYEOUPAS == 'PAYE') & (df.classification == 'PAYE')])\n",
    "    FP = len(df[(df.ENIMPAYEOUPAS == 'IMPAYE') & (df.classification == 'PAYE')])\n",
    "    FN = len(df[(df.ENIMPAYEOUPAS == 'PAYE') & (df.classification == 'IMPAYE')])\n",
    "    TN = len(df[(df.ENIMPAYEOUPAS == 'IMPAYE') & (df.classification == 'IMPAYE')])\n",
    "\n",
    "    Precision = TP / (TP + FP)\n",
    "    Recall = TP / (TP + FN)\n",
    "    Precision, Recall\n",
    "    \n",
    "    #3- Lost, no_win\n",
    "    #Computing new matrix confusion values\n",
    "    P = df.Montant[(df.ENIMPAYEOUPAS == 'IMPAYE') & (df.classification == 'PAYE')].sum()\n",
    "\n",
    "    df_no_win = df[[\"Montant\", \"Taux\", \"Nbreech.\"]][(df.ENIMPAYEOUPAS == 'PAYE') & (df.classification == 'IMPAYE')]\n",
    "    df_no_win['Taux'] /=100\n",
    "    df_no_win[\"NoWin\"] = df_no_win.Montant * df_no_win.Taux * df_no_win['Nbreech.']/12\n",
    "\n",
    "    M = df_no_win.NoWin.sum()\n",
    "    P, M\n",
    "    \n",
    "    4# Error\n",
    "    error = (FP / (TP + FP)) + (FN / FN + TN)\n",
    "    \n",
    "    #Friedman Test\n",
    "    #f, _ = friedmanchisquare(df.kredit.values, df.classification.values)\n",
    "    \n",
    "    #resulting metric list\n",
    "    result = [(accuracy, Precision, Recall, error), (P, M), (TP, FP, FN, TN)]\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "datas = pd.read_excel('New_datas_first.xlsx', index=False)\n",
    "cols = ['Type', 'Fonction', 'Civilité', 'Sit.Matrim', 'Paysresidence', 'Dept',\n",
    "       'Interd.Chq?', 'CODEAGENCE', 'Motif',\n",
    "       'ENIMPAYEOUPAS']\n",
    "for col in cols:\n",
    "    datas[col] = datas[col].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Execution\n",
    "random.seed(30)\n",
    "#datas = pd.read_excel('New_datas_first.xlsx', index=False)\n",
    "train, test = train_test_split(datas, 0.3)\n",
    "dt, df = bagging(train, test,\"entropy\", 100, 0.8)\n",
    "\n",
    "# Saving datas\n",
    "df.to_excel('classic_entropy.xlsx', index=False)\n",
    "compute_metrics(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(330987679.75, 19376215.741666667)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing new matrix confusion values\n",
    "P = df.Montant[(df.ENIMPAYEOUPAS == 'IMPAYE') & (df.classification == 'PAYE')].sum()\n",
    "\n",
    "df_no_win = df[[\"Montant\", \"Taux\", \"Nbreech.\"]][(df.ENIMPAYEOUPAS == 'PAYE') & (df.classification == 'IMPAYE')]\n",
    "df_no_win['Taux'] /=100\n",
    "df_no_win[\"NoWin\"] = df_no_win.Montant * df_no_win.Taux * df_no_win['Nbreech.']/12\n",
    "\n",
    "M = df_no_win.NoWin.sum()\n",
    "P, M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9873612823674476, 0.9804101622283441)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TP = len(df[(df.ENIMPAYEOUPAS == 'PAYE') & (df.classification == 'PAYE')])\n",
    "FP = len(df[(df.ENIMPAYEOUPAS == 'IMPAYE') & (df.classification == 'PAYE')])\n",
    "FN = len(df[(df.ENIMPAYEOUPAS == 'PAYE') & (df.classification == 'IMPAYE')])\n",
    "\n",
    "Precision = TP / (TP + FP)\n",
    "Recall = TP / (TP + FN)\n",
    "Precision, Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6406, 82, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.to_excel('classic_entropy.xlsx', index=False)\n",
    "TP, FP, FN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('results/classic_entropy_veh.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel('results/dataset_2/DT1- Entropy.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4515654557.0, 23109586.49, 159569562.0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Computing new matrix confusion values\n",
    "#P = df.montant[(df.STATUT == \"IMPAYE\") & (df.classification == \"PAYE\")].sum()\n",
    "\n",
    "df_lost = df[[\"montant\", \"durée_en_mois\"]][(df.STATUT == \"IMPAYE\") & (df.classification == \"PAYE\")]\n",
    "df['no_lost'] = df_lost.montant\n",
    "\n",
    "P = df.no_lost.sum()\n",
    "\n",
    "R = df.montant[(df.STATUT == \"PAYE\") & (df.classification == \"IMPAYE\")].sum()\n",
    "df_no_win = df[[\"montant\", \"taux\", \"durée_en_mois\"]][(df.STATUT == \"PAYE\") & (df.classification == \"IMPAYE\")]\n",
    "df_no_win['taux'] /=100\n",
    "df_no_win[\"NoWin\"] = df_no_win.montant * df_no_win.taux\n",
    "\n",
    "M = df_no_win.NoWin.sum()\n",
    "P, M, R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Somes others helpers values\n",
    "def compute_years(value):\n",
    "    words = value.split()\n",
    "    \n",
    "    final_value = 0\n",
    "    \n",
    "    for word in words:\n",
    "        number = int(word[0])\n",
    "        \n",
    "        if(word[1] == 'y'):\n",
    "            number *= 12 \n",
    "        \n",
    "        final_value += number\n",
    "    return final_value\n",
    "\n",
    "#Removing spaces\n",
    "def remove_space(words):\n",
    "        \n",
    "    #resulting word\n",
    "    concat_word = ''\n",
    "    #processing\n",
    "    for word in words.split():\n",
    "        concat_word  += word\n",
    "    print(concat_word)\n",
    "    return concat_word\n",
    "\n",
    "\n",
    "#renaming columns\n",
    "def process_file(data):\n",
    "    \n",
    "    for col in data.columns.tolist()[1:21]:\n",
    "        #new_col = col.split()\n",
    "        \n",
    "        #Rename columns with white spaces\n",
    "        #new_col = remove_space(col)\n",
    "        #data = data.rename(columns={col: new_col})\n",
    "        \n",
    "        #Process all datas by removing all spaces\n",
    "        data[col] = data[col].apply(remove_space)\n",
    "        #df.apply(classify_sample, axis=1, args=(tree,))\n",
    "        \n",
    "    \n",
    "    return data\n",
    "\n",
    "def process_data(train):\n",
    "    #train['Rate'] = 100 - train.ltv\n",
    "    \n",
    "    #train['Date.of.Birth'] = ((pd.Timestamp.now().normalize() - pd.to_datetime(train['Date.of.Birth'], errors = 'coerce'))/np.timedelta64(1, 'Y')).astype(int)\n",
    "    #train['DisbursalDate'] = ((pd.Timestamp.now().normalize() - pd.to_datetime(train['DisbursalDate'], errors = 'coerce'))/np.timedelta64(1, 'M')).astype(int)\n",
    "    \n",
    "    #train.loc[train['Date.of.Birth'] < 0, 'Date.of.Birth'] = train['Date.of.Birth'].median()\n",
    "    #train.loc[train['DisbursalDate'] < 0, 'DisbursalDate'] = train['DisbursalDate'].median()\n",
    "    \n",
    "    #train['Employment.Type'] = train['Employment.Type'].astype(str)\n",
    "    #train['PERFORM_CNS.SCORE.DESCRIPTION'] = train['PERFORM_CNS.SCORE.DESCRIPTION'].astype(str)\n",
    "    \n",
    "    # Delete useless value\n",
    "    #train = train.drop(['PRI.CURRENT.BALANCE'], axis=1)\n",
    "    \n",
    "    for col in ['SEC.NO.OF.ACCTS', 'SEC.ACTIVE.ACCTS', 'SEC.OVERDUE.ACCTS',\n",
    "       'NEW.ACCTS.IN.LAST.SIX.MONTHS', 'DELINQUENT.ACCTS.IN.LAST.SIX.MONTHS',\n",
    "       'AVERAGE.ACCT.AGE', 'CREDIT.HISTORY.LENGTH', 'NO.OF_INQUIRIES', 'branch_id', 'manufacturer_id', 'Date.of.Birth',\n",
    "       'Employment.Type', 'State_ID', 'MobileNo_Avl_Flag', 'Aadhar_flag',\n",
    "       'PERFORM_CNS.SCORE', 'PERFORM_CNS.SCORE.DESCRIPTION', 'PRI.NO.OF.ACCTS',\n",
    "       'PRI.ACTIVE.ACCTS', 'PRI.OVERDUE.ACCTS']:\n",
    "        train[col] = train[col].astype(str)\n",
    "    \n",
    "    #train['AVERAGE.ACCT.AGE'] = train['AVERAGE.ACCT.AGE'].apply(remove_space)\n",
    "    #train['CREDIT.HISTORY.LENGTH'] = train['CREDIT.HISTORY.LENGTH'].apply(remove_space)\n",
    "\n",
    "    \n",
    "    #Change Rate position\n",
    "    #columns = train.columns.tolist()\n",
    "    #columns.remove('loan_default')\n",
    "    #columns.append('loan_default')\n",
    "    #train = train[columns]\n",
    "    \n",
    "    #train = process_file(train)\n",
    "    \n",
    "    return train\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
